{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc81a150-0ecb-47fd-b911-da1d3ebc0cc6",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c30d94-a0c8-41d4-a11a-5fa9f2922164",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to scale numeric features in a dataset to a specific range, typically between 0 and 1. It rescales the values of the features, preserving the original distribution and relationships between the data points.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the original feature value,\n",
    "- \\( X_{\\text{min}} \\) is the minimum value of the feature in the dataset,\n",
    "- \\( X_{\\text{max}} \\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "Min-Max scaling is beneficial when features have different scales, and some algorithms might give undue importance to features with larger magnitudes. By scaling features to a common range, Min-Max scaling helps algorithms converge faster and improves their performance, especially for algorithms sensitive to feature scales like k-nearest neighbors (KNN) and neural networks.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset containing two features: house area (in square feet) and house price (in dollars). The values of these features are as follows:\n",
    "\n",
    "- House area: [1500, 2000, 1800, 2200, 2500]\n",
    "- House price: [200000, 250000, 220000, 280000, 300000]\n",
    "\n",
    "To apply Min-Max scaling to these features, we first calculate the minimum and maximum values for each feature:\n",
    "\n",
    "- House area: \\( X_{\\text{min}} = 1500 \\), \\( X_{\\text{max}} = 2500 \\)\n",
    "- House price: \\( X_{\\text{min}} = 200000 \\), \\( X_{\\text{max}} = 300000 \\)\n",
    "\n",
    "Next, we apply the Min-Max scaling formula to each feature:\n",
    "\n",
    "For house area:\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - 1500}{2500 - 1500} \\]\n",
    "\n",
    "For house price:\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - 200000}{300000 - 200000} \\]\n",
    "\n",
    "After applying the scaling, the scaled values will fall within the range of 0 to 1 for both features. These scaled features can then be used for further analysis or model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37fa55-d334-4852-baf9-377b7d93325f",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16df9ef-cb02-4cf7-9229-cf95079590db",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization or feature normalization, is a method used to scale features in a dataset to have a unit norm, typically L2 norm (Euclidean norm). This technique rescales each feature vector to have a length of 1 while preserving the direction of the vector.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X}{\\|X\\|_2} \\]\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the original feature vector,\n",
    "- \\( \\|X\\|_2 \\) is the L2 norm of the feature vector.\n",
    "\n",
    "Unit Vector scaling ensures that each feature contributes equally to the distance computation and can be useful when features have different units or scales. It is commonly used in machine learning algorithms that rely on distance measures, such as k-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "\n",
    "Difference between Unit Vector scaling and Min-Max scaling:\n",
    "\n",
    "1. **Normalization vs. Rescaling**:\n",
    "   - Unit Vector scaling normalizes the feature vectors to have a unit norm (length), whereas Min-Max scaling rescales the feature values to a specific range, typically between 0 and 1.\n",
    "\n",
    "2. **Preservation of Direction**:\n",
    "   - Unit Vector scaling preserves the direction of the feature vectors while ensuring they have a unit norm. In contrast, Min-Max scaling only rescales the values within a range, potentially altering the direction of the vectors.\n",
    "\n",
    "3. **Impact on Distance Measures**:\n",
    "   - Unit Vector scaling ensures that each feature contributes equally to the distance computation, making it suitable for algorithms like KNN, where distances between data points are crucial. Min-Max scaling may not achieve this equal contribution unless the range is carefully chosen.\n",
    "\n",
    "Here's an example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Suppose we have a dataset with two features: house area (in square feet) and number of bedrooms. The feature vectors for two houses are as follows:\n",
    "\n",
    "- House 1: [1500 sq ft, 3 bedrooms]\n",
    "- House 2: [2000 sq ft, 4 bedrooms]\n",
    "\n",
    "To apply Unit Vector scaling to these feature vectors, we first calculate the L2 norm (Euclidean norm) for each vector:\n",
    "\n",
    "\\[ \\|X\\|_2 = \\sqrt{(1500^2 + 3^2)} \\] for House 1\n",
    "\n",
    "\\[ \\|X\\|_2 = \\sqrt{(2000^2 + 4^2)} \\] for House 2\n",
    "\n",
    "Next, we divide each feature vector by its corresponding L2 norm to scale them to have a unit norm. The scaled feature vectors would then represent the direction of the original vectors while having a length of 1. These scaled feature vectors can be used in machine learning algorithms that rely on distance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42a877-7542-4242-8ff4-3b40cc707bba",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92b470-7946-473b-a082-d2455661b224",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving most of the important information. PCA accomplishes this by identifying the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1. **Centering the Data**:\n",
    "   - The first step in PCA is to center the data by subtracting the mean of each feature. This ensures that the data is centered around the origin.\n",
    "\n",
    "2. **Computing Covariance Matrix**:\n",
    "   - PCA computes the covariance matrix of the centered data. The covariance matrix describes the relationships between pairs of features and their variances.\n",
    "\n",
    "3. **Eigendecomposition**:\n",
    "   - PCA performs eigendecomposition on the covariance matrix to find the eigenvectors and eigenvalues. The eigenvectors represent the directions (principal components) along which the data varies the most, and the eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. **Selecting Principal Components**:\n",
    "   - PCA ranks the eigenvectors based on their corresponding eigenvalues. The principal components are selected based on the amount of variance they explain. Typically, only the top k principal components are retained, where k is the desired dimensionality of the reduced space.\n",
    "\n",
    "5. **Transforming the Data**:\n",
    "   - Finally, PCA transforms the original data into the lower-dimensional space spanned by the selected principal components. This transformation is achieved by projecting the data onto the principal component axes.\n",
    "\n",
    "PCA is commonly used in various applications, including data visualization, noise reduction, and feature extraction. By reducing the dimensionality of the data while retaining most of the variance, PCA can simplify the data representation and improve the performance of machine learning algorithms, especially in scenarios with high-dimensional data or multicollinearity among features.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Suppose we have a dataset with three features: height, weight, and age of individuals. We want to reduce the dimensionality of the data from three dimensions to two dimensions using PCA.\n",
    "\n",
    "1. **Centering the Data**:\n",
    "   - Subtract the mean of each feature from the corresponding feature values to center the data around the origin.\n",
    "\n",
    "2. **Computing Covariance Matrix**:\n",
    "   - Compute the covariance matrix of the centered data, which describes the relationships between height, weight, and age.\n",
    "\n",
    "3. **Eigendecomposition**:\n",
    "   - Perform eigendecomposition on the covariance matrix to find the eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Selecting Principal Components**:\n",
    "   - Rank the eigenvectors based on their corresponding eigenvalues. Select the top two eigenvectors as the principal components.\n",
    "\n",
    "5. **Transforming the Data**:\n",
    "   - Project the original data onto the two principal component axes to obtain the reduced-dimensional representation of the data.\n",
    "\n",
    "The resulting two-dimensional representation captures the most important information in the original dataset while reducing its dimensionality. This reduced representation can be used for visualization, analysis, or as input to machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e1349-df96-4684-b35a-ee84e0284e31",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c4a0e-19f9-4db2-8a0f-0c2396d252fc",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts. PCA can be used as a feature extraction technique to transform the original features into a new set of features called principal components. These principal components are linear combinations of the original features that capture the maximum variance in the data. By selecting a subset of the principal components, we can effectively reduce the dimensionality of the data while retaining most of the important information.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Begin by preprocessing the data, including handling missing values, scaling the features, and encoding categorical variables if necessary.\n",
    "\n",
    "2. **Applying PCA**:\n",
    "   - Apply PCA to the preprocessed data to extract the principal components. PCA identifies the directions (principal components) along which the data varies the most and projects the data onto these components.\n",
    "\n",
    "3. **Selecting Principal Components**:\n",
    "   - Select a subset of the principal components based on the amount of variance they explain or the desired dimensionality of the reduced space. The number of principal components to retain can be determined based on the cumulative explained variance or by specifying a desired percentage of variance to be retained.\n",
    "\n",
    "4. **Transforming the Data**:\n",
    "   - Transform the original data into the lower-dimensional space spanned by the selected principal components. This transformation is achieved by projecting the data onto the principal component axes.\n",
    "\n",
    "5. **Feature Extraction**:\n",
    "   - The transformed data represents the extracted features, where each principal component corresponds to a new feature. These extracted features can be used as input to machine learning algorithms for tasks such as classification, regression, or clustering.\n",
    "\n",
    "Here's an example to illustrate the concept of using PCA for feature extraction:\n",
    "\n",
    "Suppose we have a dataset with five features: height, weight, age, income, and education level of individuals. We want to reduce the dimensionality of the data by extracting new features using PCA.\n",
    "\n",
    "1. **Applying PCA**:\n",
    "   - Apply PCA to the dataset to extract the principal components. PCA identifies the directions (principal components) along which the data varies the most.\n",
    "\n",
    "2. **Selecting Principal Components**:\n",
    "   - Select the top two principal components based on the amount of variance they explain. Let's say these two principal components explain 95% of the total variance in the data.\n",
    "\n",
    "3. **Transforming the Data**:\n",
    "   - Transform the original data into the two-dimensional space spanned by the selected principal components. This transformation results in a new dataset with two extracted features representing the most important information in the original dataset.\n",
    "\n",
    "The resulting extracted features can then be used as input to machine learning algorithms for various tasks, such as clustering individuals based on their characteristics or predicting a target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925cecf-2109-4278-a778-1151dafff17d",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec79bd-6c6c-4311-9c54-843f646a314f",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, you would follow these steps:\n",
    "\n",
    "1. **Understand the Dataset**:\n",
    "   - Begin by understanding the dataset, including the meaning and range of each feature. Identify the features relevant to the recommendation system, such as price, rating, and delivery time.\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - Perform any necessary data preprocessing steps, such as handling missing values, encoding categorical variables, and removing outliers.\n",
    "\n",
    "3. **Min-Max Scaling**:\n",
    "   - Apply Min-Max scaling to the selected features to scale their values to a specific range, typically between 0 and 1. This ensures that all features are on a similar scale, which can improve the performance of recommendation algorithms.\n",
    "   - The formula for Min-Max scaling is:\n",
    "     \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "     where:\n",
    "     - \\( X \\) is the original feature value,\n",
    "     - \\( X_{\\text{min}} \\) is the minimum value of the feature,\n",
    "     - \\( X_{\\text{max}} \\) is the maximum value of the feature.\n",
    "\n",
    "4. **Example**:\n",
    "   - Let's consider three features from the dataset: price, rating, and delivery time.\n",
    "   - For each feature, calculate the minimum and maximum values:\n",
    "     - Price: \\( X_{\\text{min}} = \\$5 \\), \\( X_{\\text{max}} = \\$30 \\)\n",
    "     - Rating: \\( X_{\\text{min}} = 1 \\), \\( X_{\\text{max}} = 5 \\)\n",
    "     - Delivery time (in minutes): \\( X_{\\text{min}} = 10 \\), \\( X_{\\text{max}} = 60 \\)\n",
    "   - Apply Min-Max scaling to each feature using the formula mentioned above.\n",
    "   - For example, if the original price of a restaurant is \\$20:\n",
    "     \\[ X_{\\text{scaled}} = \\frac{20 - 5}{30 - 5} = \\frac{15}{25} = 0.6 \\]\n",
    "     Similarly, scale the rating and delivery time features.\n",
    "   - Repeat this process for all data points in the dataset.\n",
    "\n",
    "5. **Use the Scaled Data**:\n",
    "   - Once the data is scaled using Min-Max scaling, you can use it as input to build the recommendation system. The scaled features will ensure that all input features are on a similar scale, which can lead to better performance of the recommendation algorithms.\n",
    "\n",
    "By applying Min-Max scaling to features such as price, rating, and delivery time, you ensure that they are on a consistent scale, making them suitable for use in building a recommendation system for the food delivery service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139676da-c6a7-465e-907f-c68c4ea2d8bb",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67230a8-cb04-4697-878a-96c824066ed0",
   "metadata": {},
   "source": [
    "To reduce the dimensionality of the dataset containing many features for predicting stock prices using PCA (Principal Component Analysis), you would follow these steps:\n",
    "\n",
    "1. **Understand the Dataset**:\n",
    "   - Begin by understanding the dataset containing company financial data, market trends, and other relevant features. Identify the features that might influence stock prices, such as earnings, revenue, market indices, economic indicators, etc.\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - Perform necessary data preprocessing steps, including handling missing values, scaling the features, and encoding categorical variables if applicable. Ensure that the data is in a suitable format for PCA.\n",
    "\n",
    "3. **Apply PCA**:\n",
    "   - Apply PCA to the preprocessed dataset to reduce its dimensionality while preserving most of the important information. PCA identifies the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "4. **Determine the Number of Components**:\n",
    "   - Decide on the number of principal components to retain. This can be based on the desired dimensionality of the reduced dataset or the cumulative explained variance. For example, you might aim to retain 95% of the total variance.\n",
    "\n",
    "5. **Transform the Data**:\n",
    "   - Transform the original dataset into the lower-dimensional space spanned by the selected principal components. This transformation is achieved by projecting the data onto the principal component axes.\n",
    "\n",
    "6. **Feature Interpretation**:\n",
    "   - Analyze the principal components to understand which original features contribute most to each component. This can provide insights into the underlying structure of the data and help interpret the reduced-dimensional representation.\n",
    "\n",
    "7. **Model Training**:\n",
    "   - Use the reduced-dimensional dataset as input to train machine learning models for predicting stock prices. The reduced dataset should contain fewer features, which can lead to faster model training and improved model performance, especially if there were multicollinearity issues among the original features.\n",
    "\n",
    "8. **Evaluate Model Performance**:\n",
    "   - Evaluate the performance of the predictive models using appropriate evaluation metrics, such as mean squared error (MSE), root mean squared error (RMSE), or coefficient of determination (R-squared). Compare the performance of models trained on the original dataset and the reduced-dimensional dataset to assess the effectiveness of PCA in improving model performance.\n",
    "\n",
    "By following these steps, you can use PCA to effectively reduce the dimensionality of the dataset containing many features for predicting stock prices. PCA can help simplify the data representation, mitigate multicollinearity issues, and improve the efficiency and performance of predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4a17e-0a56-40c1-8dc3-17d9bb90be3e",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed200104-b87b-40b0-8ea5-22b3e9daded7",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling to transform the values to a range of -1 to 1, you need to follow these steps:\n",
    "\n",
    "1. Find the minimum and maximum values in the dataset.\n",
    "2. Apply the Min-Max scaling formula to each value in the dataset.\n",
    "\n",
    "Let's perform Min-Max scaling on the given dataset: [1, 5, 10, 15, 20].\n",
    "\n",
    "Step 1: Find the minimum and maximum values:\n",
    "- Minimum value (\\(X_{\\text{min}}\\)) = 1\n",
    "- Maximum value (\\(X_{\\text{max}}\\)) = 20\n",
    "\n",
    "Step 2: Apply the Min-Max scaling formula to each value:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - 1}{20 - 1} \\]\n",
    "\n",
    "Let's calculate the scaled values for each data point:\n",
    "\n",
    "1. For \\(X = 1\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{1 - 1}{20 - 1} = \\frac{0}{19} = 0 \\]\n",
    "\n",
    "2. For \\(X = 5\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{5 - 1}{20 - 1} = \\frac{4}{19} \\]\n",
    "\n",
    "3. For \\(X = 10\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{10 - 1}{20 - 1} = \\frac{9}{19} \\]\n",
    "\n",
    "4. For \\(X = 15\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{15 - 1}{20 - 1} = \\frac{14}{19} \\]\n",
    "\n",
    "5. For \\(X = 20\\):\n",
    "\\[ X_{\\text{scaled}} = \\frac{20 - 1}{20 - 1} = \\frac{19}{19} = 1 \\]\n",
    "\n",
    "So, the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] transformed to a range of -1 to 1 are:\n",
    "\n",
    "\\[ [-1, -\\frac{15}{19}, -\\frac{1}{19}, \\frac{14}{19}, 1] \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a8652-2930-40d5-aa55-cda0d8c4e985",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae4293-59b9-452d-beab-658139f18a7f",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on the dataset containing features such as height, weight, age, gender, and blood pressure, we need to follow these steps:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Preprocess the data, including handling missing values, encoding categorical variables (if applicable), and scaling the features. For PCA, it's essential to scale numerical features to have zero mean and unit variance.\n",
    "\n",
    "2. **Apply PCA**:\n",
    "   - Apply PCA to the preprocessed dataset to extract the principal components. PCA identifies the directions (principal components) along which the data varies the most.\n",
    "\n",
    "3. **Determine the Number of Components**:\n",
    "   - Decide on the number of principal components to retain. This decision can be based on the cumulative explained variance or the desired dimensionality of the reduced space.\n",
    "\n",
    "4. **Transform the Data**:\n",
    "   - Transform the original dataset into the lower-dimensional space spanned by the selected principal components. This transformation is achieved by projecting the data onto the principal component axes.\n",
    "\n",
    "5. **Feature Interpretation**:\n",
    "   - Analyze the principal components to understand which original features contribute most to each component. This can provide insights into the underlying structure of the data.\n",
    "\n",
    "6. **Model Training**:\n",
    "   - Use the reduced-dimensional dataset as input to train machine learning models for various tasks.\n",
    "\n",
    "The number of principal components to retain depends on several factors, including the desired dimensionality of the reduced space, the amount of explained variance, and the specific requirements of the problem at hand. Here are some considerations for choosing the number of principal components:\n",
    "\n",
    "1. **Cumulative Explained Variance**:\n",
    "   - Calculate the cumulative explained variance ratio for each additional principal component. You may choose to retain enough components to capture a high percentage (e.g., 95%) of the total variance in the data.\n",
    "\n",
    "2. **Elbow Method**:\n",
    "   - Plot the cumulative explained variance ratio against the number of principal components. Look for an \"elbow\" point, where the rate of increase in explained variance starts to decrease significantly. This point can be a good indication of the optimal number of components to retain.\n",
    "\n",
    "3. **Domain Knowledge**:\n",
    "   - Consider the interpretability of the principal components and whether they align with the underlying structure of the data. If certain features are more important or relevant to the problem, you may choose to retain more components that capture these features.\n",
    "\n",
    "4. **Computational Resources**:\n",
    "   - Consider computational constraints, especially for large datasets. Retaining fewer components can lead to faster model training and inference times.\n",
    "\n",
    "Without specific information about the dataset and its characteristics, it's challenging to determine the exact number of principal components to retain. However, a common approach is to start with retaining enough components to capture a high percentage of the total variance (e.g., 95%) and then adjust based on other considerations such as interpretability and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e86f7a-55b2-4662-a6d8-6afc9b1d4a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
